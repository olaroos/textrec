Standard Gradient Descent:
--------------------------

training uses only one cost function output to update the weights and biases.

Mini-Batch training:
--------------------

uses an average of multiple inputs/outputs of the cost function to update the weights and biases in one go.

mini-batch training of batch size 1 is SGD(Standard Gradient Descent training)

Back-propagation:
-----------------

For back propagation to work we need to make two assumptions of the cost-function.

#1 
	The cost function can be written as an average C = 1/n \sum_x C_x for individual training examples, x. 

#2 
	The cost can be written as a function of the outputs from the neural network:
	cost C = C(a^L)
