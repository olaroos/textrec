{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import theano\n",
    "import theano.tensor as T\n",
    "import theano.tensor.nnet as nnet\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def readTrainingFile(filename):\n",
    "    \n",
    "    theFile = open(filename, 'r')\n",
    "    line = theFile.readline()\n",
    "    while line != None:\n",
    "        values = line.split()\n",
    "        target = float(values[0])\n",
    "        training = np.asarray(values[1:], dtype=float)\n",
    "        line = theFile.readline()\n",
    "        \n",
    "def layer(x, w):\n",
    "    b = np.array([1], dtype=theano.config.floatX)\n",
    "    new_x = T.concatenate([x, b])\n",
    "    m = T.dot(w.T, new_x)\n",
    "    h = nnet.sigmoid(m)\n",
    "    return h\n",
    "\n",
    "def grad_desc(cost, theta):\n",
    "    alpha = 0.1 #learning rate\n",
    "    #     T.grad(function, wrt=theta)) \n",
    "    #     derive function with respect to theta\n",
    "    return theta - (alpha * T.grad(cost, wrt=theta))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = T.dvector()\n",
    "y = T.dscalar()\n",
    "\n",
    "theta1 = theano.shared(np.array(np.random.rand(30, 30), dtype=theano.config.floatX))\n",
    "theta2 = theano.shared(np.array(np.random.rand(30, 30), dtype=theano.config.floatX))\n",
    "theta3 = theano.shared(np.array(np.random.rand(31, 1), dtype=theano.config.floatX))\n",
    "\n",
    "# hidden layer\n",
    "hidl1 = layer(x, theta1)\n",
    "hidl2 = layer(hidl1, theta2)\n",
    "\n",
    "outl = T.sum(layer(hidl2, theta3))\n",
    "\n",
    "# cost expression\n",
    "fc = (outl - y)**2 \n",
    "\n",
    "cost = theano.function(inputs=[x, y], outputs=fc, updates=[\n",
    "        (theta1, grad_desc(fc, theta1)),\n",
    "        (theta2, grad_desc(fc, theta2)),\n",
    "        (theta3, grad_desc(fc, theta3))])\n",
    "\n",
    "run_forward = theano.function(inputs=[x], outputs=outl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"The sigmoid function.\"\"\"\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes):\n",
    "        \"\"\"The list ``sizes`` contains the number of neurons in the\n",
    "        respective layers of the network.  For example, if the list\n",
    "        was [2, 3, 1] then it would be a three-layer network, with the\n",
    "        first layer containing 2 neurons, the second layer 3 neurons,\n",
    "        and the third layer 1 neuron.  The biases and weights for the\n",
    "        network are initialized randomly, using a Gaussian\n",
    "        distribution with mean 0, and variance 1.  Note that the first\n",
    "        layer is assumed to be an input layer, and by convention we\n",
    "        won't set any biases for those neurons, since biases are only\n",
    "        ever used in computing the outputs from later layers.\"\"\"\n",
    "        self.a = [np.ones((y, 1)) for y in sizes[1:]]\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        \n",
    "    def feedforward(self, a):\n",
    "        \"\"\"Return the output of the network if ``a`` is input.\"\"\"\n",
    "        \"\"\"Change this, return a matrix with output values\"\"\"    \n",
    "        \n",
    "        \"\"\"WHAT TO DO HERE, I WANT TO LOOP OVER self.a,\n",
    "        A should be a reference that I can change.....\n",
    "        then wonderful things can happen\"\"\"\n",
    "        A = self.sizes\n",
    "        count = 0\n",
    "        for b, w, count in zip(self.biases, self.weights, [x for x in range(0,len(self.sizes)-1)]):\n",
    "            \"\"\"changed from sigmoid(np.dot(w, a) + b)\n",
    "            because adding elements of shape (6,) and shape (6,1) \n",
    "            becomes shape (6, 6) for some reason\"\"\"\n",
    "            a = sigmoid(np.dot(w, a).reshape(b.shape)+b)\n",
    "            A[count] = a\n",
    "            count += 1\n",
    "        self.a = A\n",
    "        return a\n",
    "\n",
    "    def outputError(self, z, y):\n",
    "        \"\"\"error = nabla_a C cdot sigmoid_prime(z)\n",
    "           error = \"\"\"\n",
    "        outputError = (z - y) * sigmoid_prime(z)\n",
    "        return outputError\n",
    "        \n",
    "    def backpropError(self, error1, w1, z0):\n",
    "        \"\"\"backpropagate error \n",
    "        newError = w^T * error cdot sigmoid_prime(z)\"\"\"\n",
    "        error0 = np.transpose(w1).dot(error1) * sigmoid_prime(z0)\n",
    "#         error0 = np.einsum('i,i->i', np.transpose(w1).dot(error1), sigmoid_prime(z0))\n",
    "        return newError\n",
    "    \n",
    "    def weightGrad(self, a0, error1):\n",
    "        \"\"\"need to repeat a0 and multiply with error or repeat error etc.\"\"\"\n",
    "        return np.dot(ao, error1)\n",
    "#         return np.einsum('i,i->i', a0, error1)\n",
    "    \n",
    "    def biasGrad(self, error):\n",
    "        \"\"\"only to remind me that this is the defined error\"\"\"\n",
    "        return error\n",
    "    \n",
    "    def trainNet(self, x, y):\n",
    "        nablaW = self.sizes[1:]\n",
    "        nablaB = self.sizes[1:]\n",
    "        out = self.feedforward(x)\n",
    "        outError = self.outputError(out, y)\n",
    "        for w1, a0 in zip(self.weights, self.a[:-1]):\n",
    "            pass\n",
    "            \"\"\"need to do stuff here\"\"\"\n",
    "            \n",
    "            \n",
    "        \"\"\"now do I want to update the weights and biases before I backprop the error\n",
    "        I would want to calculate all the errors first and then loop through \n",
    "        them and update\"\"\"\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net = Network([2, 6, 1])\n",
    "\n",
    "x = np.array([0, 1])\n",
    "y = np.array([1])\n",
    "\n",
    "net.trainNet(x, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-b9feeac48033>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.61953908  0.59959084  0.55459293  0.56265256  0.72461614  0.7265567\n",
      "   0.55596404  0.5658212   0.67270275  0.63370964]]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "1\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
